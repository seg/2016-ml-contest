{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contest entry by Wouter Kimman \n",
    "\n",
    "\n",
    "Strategy: \n",
    "----------------------------------------------\n",
    "stacking 2 layers of random forests\n",
    "\n",
    "downsampling and permutation of targeted prediction for some difficult facies\n",
    "\n",
    "I have also added Paolo Bestagini's pre-preproccessing routine, but kept mine as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import rfft\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from sqlalchemy.sql import text\n",
    "from sklearn import tree\n",
    "#from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "#import sherlock.filesystem as sfs\n",
    "#import sherlock.database as sdb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from collections import Counter, OrderedDict\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def permute_facies_nr(predicted_super, predicted0, faciesnr):\n",
    "    predicted=predicted0.copy()\n",
    "    N=len(predicted)\n",
    "    for ii in range(N):\n",
    "        if predicted_super[ii]==1:\n",
    "            predicted[ii]=faciesnr  \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarify(dataset0, facies_nr):\n",
    "    dataset=dataset0.copy()\n",
    "    mask=dataset != facies_nr\n",
    "    dataset[mask]=0\n",
    "    mask=dataset == facies_nr\n",
    "    dataset[mask]=1    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_balanced_binary(df_in, faciesnr, factor):\n",
    "    df=df_in.copy()\n",
    "    y=df['Facies'].values\n",
    "    y0=binarify(y, faciesnr)\n",
    "    df['Facies']=y0\n",
    "\n",
    "    df1=df[df['Facies']==1]\n",
    "    X_part1=df1.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    y_part1=df1['Facies'].values\n",
    "    N1=len(df1)\n",
    "\n",
    "    df2=df[df['Facies']==0]\n",
    "    X_part0=df2.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    y_part0=df2['Facies'].values\n",
    "    N2=len(df2)\n",
    "    print \"ratio now:\"\n",
    "    print float(N2)/float(N1)\n",
    "    ratio_to_keep=factor*float(N1)/float(N2)\n",
    "    print \"ratio after:\"\n",
    "    print float(N2)/(factor*float(N1))\n",
    "    dum1, X_part2, dum2, y_part2 = train_test_split(X_part0, y_part0, test_size=ratio_to_keep, random_state=42)\n",
    "\n",
    "    tmp=[X_part1, X_part2]  \n",
    "    X = pd.concat(tmp, axis=0)\n",
    "    y = np.concatenate((y_part1, y_part2))\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def phaseI_model(regime_train, correctA, go_B, clf, pred_array, pred_blind, features_blind):      \n",
    "    clf.fit(regime_train,correctA)     \n",
    "    predicted_B = clf.predict(go_B)\n",
    "    pred_array = np.vstack((predicted_B, pred_array))   \n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "    pred_blind = np.vstack((predicted_blind1, pred_blind))    \n",
    "    return pred_array, pred_blind\n",
    "\n",
    "def phaseI_model_scaled(regime_train, correctA, go_B, clf, pred_array, pred_blind, features_blind):   \n",
    "    regime_train=StandardScaler().fit_transform(regime_train)\n",
    "    go_B=StandardScaler().fit_transform(go_B)\n",
    "    features_blind=StandardScaler().fit_transform(features_blind)\n",
    "    clf.fit(regime_train,correctA)     \n",
    "    predicted_B = clf.predict(go_B)\n",
    "    pred_array = np.vstack((predicted_B, pred_array))\n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "    pred_blind = np.vstack((predicted_blind1, pred_blind))\n",
    "    return pred_array, pred_blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_structure_for_regimes(df):\n",
    "    allfeats=['GR','ILD_log10','DeltaPHI','PHIND','PE','NM_M','RELPOS']\n",
    "    data_all = []\n",
    "    for feat in allfeats:\n",
    "        dff=df.groupby('Well Name').describe(percentiles=[0.1, 0.25, .5, 0.75, 0.9]).reset_index().pivot(index='Well Name', values=feat, columns='level_1')\n",
    "        dff = dff.drop(['count'], axis=1)\n",
    "        cols=dff.columns\n",
    "        cols_new=[]\n",
    "        for ii in cols:\n",
    "            strin=feat + \"_\" + str(ii)\n",
    "            cols_new.append(strin)\n",
    "        dff.columns=cols_new \n",
    "        dff1=dff.reset_index()\n",
    "        if feat=='GR':\n",
    "            data_all.append(dff1)\n",
    "        else:\n",
    "            data_all.append(dff1.iloc[:,1:])\n",
    "    data_all = pd.concat(data_all,axis=1)\n",
    "    return data_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def magic(df):\n",
    "    df1=df.copy()\n",
    "    b, a = signal.butter(2, 0.2, btype='high', analog=False)\n",
    "    feats0=['GR','ILD_log10','DeltaPHI','PHIND','PE','NM_M','RELPOS']\n",
    "    #feats01=['GR','ILD_log10','DeltaPHI','PHIND']\n",
    "    #feats01=['DeltaPHI']\n",
    "    #feats01=['GR','DeltaPHI','PHIND']\n",
    "    feats01=['GR',]\n",
    "    feats02=['PHIND']\n",
    "    #feats02=[]\n",
    "    for ii in feats0:\n",
    "        df1[ii]=df[ii]\n",
    "        name1=ii + '_1'\n",
    "        name2=ii + '_2'\n",
    "        name3=ii + '_3'\n",
    "        name4=ii + '_4'\n",
    "        name5=ii + '_5'\n",
    "        name6=ii + '_6'\n",
    "        name7=ii + '_7'\n",
    "        name8=ii + '_8'\n",
    "        name9=ii + '_9'\n",
    "        xx1 = list(df[ii])\n",
    "        xx_mf= signal.medfilt(xx1,9)\n",
    "        x_min1=np.roll(xx_mf, 1)\n",
    "        x_min2=np.roll(xx_mf, -1)\n",
    "        x_min3=np.roll(xx_mf, 3)\n",
    "        x_min4=np.roll(xx_mf, 4)\n",
    "        xx1a=xx1-np.mean(xx1)\n",
    "        xx_fil = signal.filtfilt(b, a, xx1)        \n",
    "        xx_grad=np.gradient(xx1a) \n",
    "        x_min5=np.roll(xx_grad, 3)\n",
    "        #df1[name4]=xx_mf\n",
    "        if ii in feats01: \n",
    "            df1[name1]=x_min3\n",
    "            df1[name2]=xx_fil\n",
    "            df1[name3]=xx_grad\n",
    "            df1[name4]=xx_mf \n",
    "            df1[name5]=x_min1\n",
    "            df1[name6]=x_min2\n",
    "            df1[name7]=x_min4\n",
    "            #df1[name8]=x_min5\n",
    "            #df1[name9]=x_min2\n",
    "        if ii in feats02:\n",
    "            df1[name1]=x_min3\n",
    "            df1[name2]=xx_fil\n",
    "            df1[name3]=xx_grad\n",
    "            #df1[name4]=xx_mf \n",
    "            df1[name5]=x_min1\n",
    "            #df1[name6]=x_min2 \n",
    "            #df1[name7]=x_min4\n",
    "    return df1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As others have done, this is Paolo Bestagini's pre-preoccessing routine \n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "#X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filename = 'training_data.csv'\n",
    "filename = 'facies_vectors.csv'\n",
    "training_data0 = pd.read_csv(filename)\n",
    "filename = 'validation_data_nofacies.csv'\n",
    "test_data = pd.read_csv(filename)\n",
    "\n",
    "#blindwell='CHURCHMAN BIBLE'\n",
    "#blindwell='LUKE G U'\n",
    "blindwell='STUART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHRIMPLIN' 'ALEXANDER D' 'SHANKLE' 'LUKE G U' 'KIMZEY A' 'CROSS H CATTLE'\n",
      " 'NOLAN' 'Recruit F9' 'NEWBY' 'CHURCHMAN BIBLE']\n"
     ]
    }
   ],
   "source": [
    "all_wells=training_data0['Well Name'].unique()\n",
    "print all_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5515\n",
      "SHRIMPLIN\n",
      "471\n",
      "using median of local\n",
      "ALEXANDER D\n",
      "466\n",
      "using median of total\n",
      "SHANKLE\n",
      "449\n",
      "using median of local\n",
      "LUKE G U\n",
      "461\n",
      "using median of local\n",
      "KIMZEY A\n",
      "439\n",
      "using median of total\n",
      "CROSS H CATTLE\n",
      "501\n",
      "using median of local\n",
      "NOLAN\n",
      "415\n",
      "using median of local\n",
      "Recruit F9\n",
      "80\n",
      "using median of local\n",
      "NEWBY\n",
      "463\n",
      "using median of local\n",
      "CHURCHMAN BIBLE\n",
      "404\n",
      "using median of local\n",
      "4149\n",
      "4149\n"
     ]
    }
   ],
   "source": [
    "# what to do with the naans\n",
    "training_data1=training_data0.copy()\n",
    "me_tot=training_data1['PE'].median()\n",
    "print me_tot\n",
    "for well in all_wells:\n",
    "    df=training_data0[training_data0['Well Name'] == well] \n",
    "    print well\n",
    "    print len(df)\n",
    "    df0=df.dropna()\n",
    "    #print len(df0)\n",
    "    if len(df0) > 0:\n",
    "        print \"using median of local\"\n",
    "        me=df['PE'].median()\n",
    "        df=df.fillna(value=me)\n",
    "    else:\n",
    "        print \"using median of total\"\n",
    "        df=df.fillna(value=me_tot)\n",
    "    training_data1[training_data0['Well Name'] == well] =df\n",
    "    \n",
    "\n",
    "print len(training_data1)\n",
    "df0=training_data1.dropna()\n",
    "print len(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4149\n",
      "4149\n",
      "4149\n",
      "4143\n"
     ]
    }
   ],
   "source": [
    "#remove outliers\n",
    "df=training_data1.copy()\n",
    "print len(df)\n",
    "df0=df.dropna()\n",
    "print len(df0)\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "#df=pd.DataFrame(np.random.randn(20,3))\n",
    "#df.iloc[3,2]=5\n",
    "print len(df1)\n",
    "df2=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "print len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2a=df2[df2['Well Name'] != 'Recruit F9'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all=create_structure_for_regimes(df2a)\n",
    "data_test=create_structure_for_regimes(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collating the Data:\n",
    "-----------------------------------------------------------\n",
    "based on the regimes we determined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n",
      "1707\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "# based on kmeans clustering\n",
    "data=[]\n",
    "df = training_data0[training_data0['Well Name'] == 'ALEXANDER D'] \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'LUKE G U']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'CROSS H CATTLE']  \n",
    "data.append(df)\n",
    "Regime_1 = pd.concat(data, axis=0)\n",
    "print len(Regime_1)\n",
    "\n",
    "data=[]\n",
    "df = training_data0[training_data0['Well Name'] == 'KIMZEY A']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'NOLAN']\n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'CHURCHMAN BIBLE']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'SHANKLE'] \n",
    "data.append(df)\n",
    "Regime_2 = pd.concat(data, axis=0)\n",
    "print len(Regime_2)\n",
    "\n",
    "data=[]\n",
    "\n",
    "df = training_data0[training_data0['Well Name'] == 'SHRIMPLIN']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'NEWBY']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'Recruit F9']  \n",
    "data.append(df)\n",
    "Regime_3 = pd.concat(data, axis=0)\n",
    "print len(Regime_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into 2 parts:**\n",
    "\n",
    "from A We will make initial predictions\n",
    "\n",
    "from B we will make the final prediction(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 0:\n",
    "---------------------------------\n",
    "-Create predictions specifically for the most difficult facies\n",
    "\n",
    "-For this stage we focus on TP and FP only: We want only a few predictions that are\n",
    "likely to be correct to edge the f1 prediction up slightly at the end\n",
    "\n",
    "-For each facies i consider the samples binary 0 or 1 and downsample the zeros to \n",
    "get a more even distribution. However, I found the results change quite a bit depending \n",
    "on the degree of downsampling. As a type of dumb-men's L-curve analysis I varied this to the \n",
    "point where the nr of predictions (1) doesn't change much more\n",
    "\n",
    "-also, based on the similarity to the other wells we have an 'indiction' on how much of the different facies we can expect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "**training for facies 9 specifically**\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df0 = test_data[test_data['Well Name'] == blindwell] \n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#df0 = training_data0[training_data0['Well Name'] == blindwell]  \n",
    "#df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "blind=magic(df1a)\n",
    "\n",
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "19.0434782609\n",
      "kk is 1, nr of predictions for this regime is 51\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "9.52173913043\n",
      "kk is 2, nr of predictions for this regime is 12\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "6.34782608696\n",
      "kk is 3, nr of predictions for this regime is 9\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "4.76086956522\n",
      "kk is 4, nr of predictions for this regime is 8\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "3.80869565217\n",
      "kk is 5, nr of predictions for this regime is 6\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "3.17391304348\n",
      "kk is 6, nr of predictions for this regime is 5\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "2.72049689441\n",
      "kk is 7, nr of predictions for this regime is 5\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "2.38043478261\n",
      "kk is 8, nr of predictions for this regime is 5\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "2.11594202899\n",
      "kk is 9, nr of predictions for this regime is 4\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "#X, y = make_balanced_binary(all1, 9,6)\n",
    "for kk in range(1,10):\n",
    "    X, y = make_balanced_binary(all1, 9,kk)\n",
    "#============================================================\n",
    "    correct_train=y\n",
    "\n",
    "    #clf = RandomForestClassifier(max_depth = 6, n_estimators=1600)\n",
    "    clf = RandomForestClassifier(max_depth = 6, n_estimators=800)\n",
    "    clf.fit(X,correct_train)\n",
    "\n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "\n",
    "    predicted_regime9=predicted_blind1.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime9)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "**training for facies 1 specifically**\n",
    "________________________\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "11.4594594595\n",
      "-------\n",
      "kk is 1, nr of predictions for this regime is 77\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "5.72972972973\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 36\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "3.81981981982\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 22\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "2.86486486486\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 4\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "2.29189189189\n",
      "-------\n",
      "kk is 5, nr of predictions for this regime is 4\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "\n",
    "for kk in range(1,6):\n",
    "#for kk in range(1,6): \n",
    "    X, y = make_balanced_binary(all1, 1,kk)\n",
    "    #============================================================\n",
    "\n",
    "    #=============================================\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "                                        \n",
    "\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = svm.SVC(decision_function_shape='ovo')\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 3:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "        \n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-1] == 0 and predicted_blind[ii+1] == 0:\n",
    "            predicted_blind[ii]=0\n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-1] == 0 and predicted_blind[ii+2] == 0:\n",
    "            predicted_blind[ii]=0        \n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-2] == 0 and predicted_blind[ii+1] == 0:\n",
    "            predicted_blind[ii]=0     \n",
    "    #####################################    \n",
    "\n",
    "    print \"-------\"\n",
    "    predicted_regime1=predicted_blind.copy()\n",
    "\n",
    "    #print(\"%c is my %s letter and my number %d number is %.5f\" % ('X', 'favorite', 1, .14))\n",
    " \n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime1)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training for facies 5 specifically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "13.8709677419\n",
      "-------\n",
      "kk is 1, nr of predictions for this regime is 84\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "6.93548387097\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 35\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "4.62365591398\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 29\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "3.46774193548\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 22\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "2.77419354839\n",
      "-------\n",
      "kk is 5, nr of predictions for this regime is 26\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "2.31182795699\n",
      "-------\n",
      "kk is 6, nr of predictions for this regime is 18\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "1.98156682028\n",
      "-------\n",
      "kk is 7, nr of predictions for this regime is 11\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "1.73387096774\n",
      "-------\n",
      "kk is 8, nr of predictions for this regime is 12\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "1.54121863799\n",
      "-------\n",
      "kk is 9, nr of predictions for this regime is 11\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "for kk in range(1,10):\n",
    "#for kk in range(2,4): \n",
    "    X, y = make_balanced_binary(all1, 5,kk)\n",
    "    #X, y = make_balanced_binary(all1, 5,13)\n",
    "    #============================================================\n",
    "\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "    #=============================================                                        \n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=4,algorithm='brute')\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5,leaf_size=10)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind5 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)    \n",
    "    predicted_blind6 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4+predicted_blind5+predicted_blind6\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 4:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "\n",
    "    print \"-------\"\n",
    "    predicted_regime5=predicted_blind.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime5)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training for facies 7 specifically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "15.9642857143\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 64\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "10.6428571429\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 44\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "7.98214285714\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 41\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "6.38571428571\n",
      "-------\n",
      "kk is 5, nr of predictions for this regime is 27\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "5.32142857143\n",
      "-------\n",
      "kk is 6, nr of predictions for this regime is 23\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "4.5612244898\n",
      "-------\n",
      "kk is 7, nr of predictions for this regime is 22\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.99107142857\n",
      "-------\n",
      "kk is 8, nr of predictions for this regime is 23\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.54761904762\n",
      "-------\n",
      "kk is 9, nr of predictions for this regime is 18\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.19285714286\n",
      "-------\n",
      "kk is 10, nr of predictions for this regime is 15\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.9025974026\n",
      "-------\n",
      "kk is 11, nr of predictions for this regime is 11\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.66071428571\n",
      "-------\n",
      "kk is 12, nr of predictions for this regime is 12\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.45604395604\n",
      "-------\n",
      "kk is 13, nr of predictions for this regime is 8\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.2806122449\n",
      "-------\n",
      "kk is 14, nr of predictions for this regime is 9\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "for kk in range(2,15):\n",
    "    X, y = make_balanced_binary(all1, 7,kk)\n",
    "    #X, y = make_balanced_binary(all1, 7,13)\n",
    "    #============================================================\n",
    "\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "    #=============================================                                        \n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=4,algorithm='brute')\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5,leaf_size=10)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind5 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)    \n",
    "    predicted_blind6 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4+predicted_blind5+predicted_blind6\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 5:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "\n",
    "\n",
    "    #####################################    \n",
    "    print \"-------\"\n",
    "    predicted_regime7=predicted_blind.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime7)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PHASE Ib \n",
    "======================================\n",
    "**PREPARE THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(Regime_1, Regime_2, Regime_3, test_data, w1, w2,w3):\n",
    "    df0=Regime_1.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime1A_train, regime1B_train, regime1A_test, regime1B_test = train_test_split(Xtot_train, y, test_size=w1, random_state=42)\n",
    "\n",
    "    df0=Regime_2.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime2A_train, regime2B_train, regime2A_test, regime2B_test = train_test_split(Xtot_train, y, test_size=w2, random_state=42)\n",
    "\n",
    "\n",
    "    df0=Regime_3.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime3A_train, regime3B_train, regime3A_test, regime3B_test = train_test_split(Xtot_train, y, test_size=w3, random_state=42)\n",
    "\n",
    "\n",
    "    #df0 = training_data0[training_data0['Well Name'] == blindwell]\n",
    "    #df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df0 = test_data[test_data['Well Name'] == blindwell] \n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    #df2a=df1a\n",
    "    X0blind = df2a[feature_names0].values\n",
    "\n",
    "    blind=df1a\n",
    "    #correct_facies_labels = blind['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = blind[feature_names].values\n",
    "    well = blind['Well Name'].values\n",
    "    depth = blind['Depth'].values\n",
    "    X2blind,  padded_rows = augment_features(X1, well, depth)\n",
    "\n",
    "    features_blind=np.column_stack((X0blind,X2blind))\n",
    "#=======================================================\n",
    "    main_regime=regime2A_train\n",
    "    other1=regime1A_train\n",
    "    other2=regime3A_train\n",
    "\n",
    "    main_test=regime2A_test\n",
    "    other1_test=regime1A_test\n",
    "    other2_test=regime3A_test\n",
    "\n",
    "    go_B=np.concatenate((regime1B_train, regime2B_train, regime3B_train))\n",
    "    correctB=np.concatenate((regime1B_test, regime2B_test, regime3B_test))\n",
    "#     #===================================================\n",
    "    train1= np.concatenate((main_regime, other1, other2))\n",
    "    correctA1=np.concatenate((main_test, other1_test, other2_test))\n",
    "#     #=================================================== \n",
    "#     train2= np.concatenate((main_regime, other2))\n",
    "#     correctA2=np.concatenate((main_test, other2_test))\n",
    "#     #===================================================\n",
    "\n",
    "    #===================================================\n",
    "    #train1=main_regime\n",
    "    #correctA1=main_test\n",
    "    train2=other1\n",
    "    correctA2=other1_test   \n",
    "    train3=other2\n",
    "    correctA3=other2_test   \n",
    "\n",
    "    return train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPARE THE DATA FOR SERIAL MODELLING**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create several predictions, varying the dataset and the technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_phaseI(train1,train2,train3,correctA1,correctA2,correctA3,correctB, go_B, features_blind):    \n",
    "    pred_array=0*correctB\n",
    "    pred_blind=np.zeros(len(features_blind))\n",
    "\n",
    "    print \"rf1\"\n",
    "    clf = RandomForestClassifier(max_depth = 5, n_estimators=2600, random_state=1)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    clf = RandomForestClassifier(max_depth = 15, n_estimators=3000)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)    \n",
    "#     pred_array, pred_blind=phaseI_model(train2, correctA2, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "#     pred_array, pred_blind=phaseI_model(train3, correctA3, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    clf = RandomForestClassifier(n_estimators=1200, max_depth = 15, criterion='entropy',\n",
    "                                 max_features=10, min_samples_split=25, min_samples_leaf=5,\n",
    "                                 class_weight='balanced', random_state=1)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    #pred_array, pred_blind=phaseI_model(train2, correctA2, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    #pred_array, pred_blind=phaseI_model(train3, correctA3, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    return pred_array, pred_blind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase II:\n",
    "---------------------------------------------\n",
    "Stacking the predictions from phase Ib. \n",
    "New predictions from data B\n",
    "\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First prediction of B data without Phase I input:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the initial predictions as features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a new prediction, with the best model on the full dataset B:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data:\n",
      "162\n",
      "running phase I:\n",
      "rf1\n",
      "prediction phase II:\n",
      "prediction phase II-stacked:\n",
      "(162, 41)\n"
     ]
    }
   ],
   "source": [
    "w1=0.05\n",
    "w2=0.05\n",
    "w3=0.05\n",
    "print \"preparing data:\"\n",
    "#train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind=prepare_data(Regime_1, Regime_2, Regime_3, training_data0, w1, w2,w3)\n",
    "train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind=prepare_data(Regime_1, Regime_2, Regime_3, test_data, w1, w2,w3)\n",
    "print(len(correctB))\n",
    "print \"running phase I:\"\n",
    "pred_array, pred_blind = run_phaseI(train1,train2,train3,correctA1,correctA2, correctA3, correctB, go_B, features_blind)\n",
    "print \"prediction phase II:\"\n",
    "clf = RandomForestClassifier(max_depth = 8, n_estimators=3000, max_features=10, criterion='entropy',class_weight='balanced')\n",
    "#clf = RandomForestClassifier(max_depth = 5, n_estimators=300, max_features=10, criterion='entropy',class_weight='balanced')\n",
    "#clf = RandomForestClassifier(n_estimators=1200, max_depth = 15, criterion='entropy',\n",
    "#                             max_features=10, min_samples_split=25, min_samples_leaf=5,\n",
    "#                             class_weight='balanced', random_state=1)\n",
    "#clf = RandomForestClassifier(n_estimators=1200, max_depth = 5, criterion='entropy',\n",
    "#                             max_features=10, min_samples_split=25, min_samples_leaf=5,\n",
    "#                             class_weight='balanced', random_state=1)\n",
    "clf.fit(go_B,correctB)\n",
    "predicted_blind_PHASE_I = clf.predict(features_blind)\n",
    "\n",
    "print \"prediction phase II-stacked:\"\n",
    "pa=pred_array[:len(pred_array)-1]\n",
    "go_B_PHASE_II=np.concatenate((pa, go_B.transpose())).transpose()\n",
    "pa1=np.median(pa,axis=0)\n",
    "go_B_PHASE_II=np.column_stack((go_B_PHASE_II,pa1))\n",
    "print go_B_PHASE_II.shape\n",
    "feat=pred_blind[:len(pred_blind)-1]\n",
    "features_blind_PHASE_II=np.concatenate((feat, features_blind.transpose())).transpose()\n",
    "feat1=np.median(feat,axis=0)\n",
    "features_blind_PHASE_II=np.column_stack((features_blind_PHASE_II,feat1))\n",
    "\n",
    "#second pred\n",
    "clf.fit(go_B_PHASE_II,correctB)\n",
    "predicted_blind_PHASE_II = clf.predict(features_blind_PHASE_II)\n",
    "\n",
    "#print \"finished\"\n",
    "#out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_I, average = 'micro')\n",
    "#print \" f1 score on the prediction of blind:\"\n",
    "#print out_f1\n",
    "#out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_II, average = 'micro')\n",
    "#print \" f1 score on the prediction of blind:\"\n",
    "#print out_f1\n",
    "#print \"finished\"\n",
    "#print \"-----------------------------\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permute facies based on earlier predictions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "9\n",
      "4\n",
      "4\n",
      "values changed:\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(sum(predicted_regime5))\n",
    "predicted_blind_PHASE_IIa=permute_facies_nr(predicted_regime5, predicted_blind_PHASE_II, 5)\n",
    "print(sum(predicted_regime7))\n",
    "predicted_blind_PHASE_IIb=permute_facies_nr(predicted_regime7, predicted_blind_PHASE_IIa, 7)\n",
    "print(sum(predicted_regime1))\n",
    "predicted_blind_PHASE_IIc=permute_facies_nr(predicted_regime1, predicted_blind_PHASE_IIb, 1)\n",
    "print(sum(predicted_regime9))\n",
    "predicted_blind_PHASE_III=permute_facies_nr(predicted_regime9, predicted_blind_PHASE_IIc, 9)\n",
    "\n",
    "\n",
    "print \"values changed:\"\n",
    "\n",
    "print len(predicted_blind_PHASE_II)-np.count_nonzero(predicted_blind_PHASE_III==predicted_blind_PHASE_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 7, 8, 8,\n",
       "       8, 6, 6, 6, 5, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 4, 6,\n",
       "       6, 6, 8, 8, 8, 8, 8, 7, 7, 7, 7, 8, 9, 9, 9, 9, 9, 8, 8, 8, 8, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 8, 8, 8, 8, 6,\n",
       "       6, 6, 6, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 8, 8, 8, 8, 5, 5, 8, 8, 8, 8, 8,\n",
       "       8, 8, 6, 6, 7, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 6, 8, 2, 2,\n",
       "       2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 9, 9, 8, 8, 7, 7, 8, 8, 8, 6, 6, 6, 6, 5, 2, 2, 3,\n",
       "       3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3,\n",
       "       3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 8, 3, 3, 3, 3, 8, 8, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 6, 8, 8, 8, 8, 8, 9, 9,\n",
       "       9, 7, 8, 8, 8, 6, 6, 6, 6, 6, 6, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 7, 7,\n",
       "       7, 7, 7, 8, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 6, 6, 6, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_blind_STUART=predicted_blind_PHASE_III\n",
    "predicted_blind_STUART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 2),\n",
       "             (2, 129),\n",
       "             (3, 44),\n",
       "             (4, 35),\n",
       "             (5, 36),\n",
       "             (6, 87),\n",
       "             (7, 30),\n",
       "             (8, 102),\n",
       "             (9, 9)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_I)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 1),\n",
       "             (2, 115),\n",
       "             (3, 59),\n",
       "             (4, 46),\n",
       "             (5, 41),\n",
       "             (6, 73),\n",
       "             (7, 23),\n",
       "             (8, 106),\n",
       "             (9, 10)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_II)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 5),\n",
       "             (2, 111),\n",
       "             (3, 59),\n",
       "             (4, 45),\n",
       "             (5, 44),\n",
       "             (6, 72),\n",
       "             (7, 30),\n",
       "             (8, 98),\n",
       "             (9, 10)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_III)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_blind_CRAWFORD=\n",
    "([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 7, 5, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4,\n",
    "       4, 4, 4, 7, 4, 4, 4, 4, 4, 4, 4, 6, 5, 6, 6, 6, 6, 5, 4, 4, 4, 4, 6,\n",
    "       8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4,\n",
    "       4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 6, 6, 4,\n",
    "       4, 4, 6, 6, 6, 6, 6, 8, 8, 3, 3, 8, 8, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8,\n",
    "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
    "       6, 5, 8, 8, 6, 6, 6, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "       2, 2, 2, 2, 3, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
    "       7, 7, 7, 7, 7, 8, 7, 6, 8, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "       2, 2, 2, 2, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 2,\n",
    "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 8, 8, 8, 8,\n",
    "       8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "       3, 3, 3, 8, 8, 8, 8, 8, 5, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
    "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
    "       7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 3, 2, 2,\n",
    "       2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
